{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wikipediaapi\n",
    "import random\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import collections\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "from datasets import load_dataset, load_metric\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "     \n",
    "\n",
    "transformers.__version__\n",
    "     \n",
    "'4.9.2'\n",
    "\n",
    "impossible_answer = True\n",
    "model_checkpoint = \"indolem/indobert-base-uncased\"\n",
    "batch_size = 16\n",
    "     \n",
    "Runtime Settings\n",
    "\n",
    "!nvidia-smi\n",
    "     \n",
    "Wed Aug 11 09:59:02 2021       \n",
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                               |                      |               MIG M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
    "| N/A   65C    P8    14W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "                                                                               \n",
    "+-----------------------------------------------------------------------------+\n",
    "| Processes:                                                                  |\n",
    "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
    "|        ID   ID                                                   Usage      |\n",
    "|=============================================================================|\n",
    "|  No running processes found                                                 |\n",
    "+-----------------------------------------------------------------------------+\n",
    "Data Preparation\n",
    "Load Dataset\n",
    "\n",
    "\n",
    "with open('BACKEND/dev-v2.0.json') as f:\n",
    "    dev = json.load(f)\n",
    "\n",
    "with open('BACKEND/train-v2.0.json') as f:\n",
    "    train = json.load(f)\n",
    "     \n",
    "\n",
    "def format(content):\n",
    "    hf_data = []\n",
    "    for data in content[\"data\"]:\n",
    "        title = data[\"title\"]\n",
    "        for paragraph in data[\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"]\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                fill = {\n",
    "                    \"id\":  qa[\"id\"],\n",
    "                    \"title\": title,\n",
    "                    \"context\": context,\n",
    "                    \"question\": qa[\"question\"],\n",
    "                    \"answers\": {\"answer_start\": [], \"text\": []}\n",
    "                }\n",
    "                answers = qa[\"plausible_answers\"] if qa[\"is_impossible\"] else qa[\"answers\"]\n",
    "                for answer in answers:\n",
    "                    fill[\"answers\"][\"answer_start\"].append(answer[\"answer_start\"])\n",
    "                    fill[\"answers\"][\"text\"].append(answer[\"text\"])\n",
    "\n",
    "                hf_data.append(fill)\n",
    "\n",
    "    return hf_data\n",
    "     \n",
    "\n",
    "%%time\n",
    "dev_data = format(dev)\n",
    "train_train = format(train)\n",
    "     \n",
    "# CPU times: user 1.46 s, sys: 12.8 ms, total: 1.47 s\n",
    "# Wall time: 1.48 s\n",
    "\n",
    "datasets = DatasetDict({\n",
    "    'train': Dataset.from_pandas(pd.DataFrame(train_data)),\n",
    "    'validation': Dataset.from_pandas(pd.DataFrame(dev_data))\n",
    "})\n",
    "     \n",
    "print(datasets)\n",
    "     \n",
    "# DatasetDict({\n",
    "#     train: Dataset({\n",
    "#         features: ['id', 'title', 'context', 'question', 'answers'],\n",
    "#         num_rows: 130318\n",
    "#     })\n",
    "#     validation: Dataset({\n",
    "#         features: ['id', 'title', 'context', 'question', 'answers'],\n",
    "#         num_rows: 11858\n",
    "#     })\n",
    "# })\n",
    "\n",
    "# datasets['train']['id'][0]\n",
    "     \n",
    "# '56be85543aeaaa14008c9063'\n",
    "# Data Analyzation\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame((np.asarray(dataset)[picks]).tolist())\n",
    "\n",
    "    df['answer_start'] = [i['answer_start'] for i in df['answers']]\n",
    "    df['answer_text'] = [i['text'] for i in df['answers']]\n",
    "\n",
    "    del df['answers']\n",
    "\n",
    "    return df\n",
    "     \n",
    "\n",
    "display(HTML(show_random_elements(dev_data).to_html()))\n",
    "     \n",
    "id\ttitle\tcontext\tquestion\tanswer_start\tanswer_text\n",
    "0\t5ad2685dd7d075001a429276\tOxygen\tTerkonsentrasi O2 akan memungkinkan pembakaran untuk melanjutkan cepat dan energik. Pipa baja dan pembuluh penyimpanan digunakan untuk menyimpan dan mengirimkan baik oksigen gas dan cair akan bertindak sebagai bahan bakar; dan karena itu desain dan pembuatan sistem O2 membutuhkan pelatihan khusus untuk memastikan bahwa sumber pengapian diminimalkan. Kebakaran yang menewaskan kru Apollo 1 dalam uji coba peluncuran menyebar begitu cepat karena kapsul itu bertekanan dengan O2 murni Tapi pada sedikit lebih dari tekanan atmosfer, bukannya 1⁄3 tekanan normal yang akan digunakan dalam misi.\tBagaimana pipa baja memungkinkan pembakaran untuk melanjutkan?\t[65]\t[cepat dan energik]\n",
    "1\t5ad4c99d5b96ef001a10a0a5\tWarsaw\tPada tahun 1529, Warsawa untuk pertama kalinya menjadi tempat duduk Jenderal Sejm, permanen dari tahun 1569. Pada tahun 1573, kota itu dinamai Konfederasi Warsawa, secara resmi menetapkan kebebasan beragama di Commonwealthuania Polandia. Karena lokasi pusat antara ibu kota Persemakmuran Kraków dan Vilnius, Warsawa menjadi ibu kota Persemakmuran dan Mahkota Kerajaan Polandia ketika Raja Sigismund III Vasa pindah dari Kraków ke Warsawa pada tahun 1596. Pada tahun-tahun berikutnya kota diperluas menuju pinggiran kota. Beberapa distrik independen didirikan, milik para bangsawan dan bangsawan, yang diperintah oleh hukum mereka sendiri. Tiga kali antara tahun 165558 kota itu dikepung dan tiga kali kota itu dijarah dan dijarah oleh pasukan Swedia, Brandenburgian dan Transylvania.\tMengapa Vilnius menjadi ibu kota Persemakmuran?\t[238]\t[Karena lokasi pusat]\n",
    "2\t5ad2b72fd7d075001a42a024\tRhine\tPada abad ke - 6, Rhine berada di perbatasan Francia. Pada abad ke-9, ia membentuk bagian dari perbatasan antara Francia Tengah dan Barat, tetapi pada abad ke-10, ia sepenuhnya berada di Imperium Romawi Suci, yang mengalir melalui Swabia, Franconia dan Lower Lorraine. Mulut Rhine, di wilayah Belanda, jatuh ke Belanda Burgundian pada abad ke-15; Belanda tetap menjadi daerah yang suka bertengkar di seluruh perang agama Eropa dan runtuhnya Imperium Romawi Suci, ketika Rhine jatuh ke Imperium Prancis Pertama dan negara - negara kliennya. Alsace di tepi kiri Rhine Atas dijual ke Burgundy oleh Archduke Sigismund dari Austria pada tahun 1469 dan akhirnya jatuh ke Prancis dalam Perang Tiga Puluh Tahun. Banyak istana bersejarah di Rhineland-Palatate membuktikan pentingnya sungai sebagai rute komersial.\tKapan kerajaan Belanda terbentuk?\t[330]\t[pada abad ke-15]\n",
    "3\t5ad40432604f3c001a3ffdbd\tYuan_dynasty\tSetelah memperkuat pemerintahannya di Cina bagian utara, Kublai mengejar kebijakan ekspansionis selaras dengan tradisi imperialisme Mongol dan Cina. Dia memperbarui drive besar-besaran melawan dinasti Song ke selatan. Kublai mengepung Xiangyang antara 1268 dan 1273, rintangan terakhir dalam perjalanannya untuk menangkap lembah Sungai Yangzi yang kaya. Suatu ekspedisi angkatan laut yang tidak berhasil dilakukan terhadap Jepang pada tahun 1274. Kublai merebut Modal Song dari Hangzhou pada tahun 1276, kota tersehat di Cina. Para loyalis yang melarikan diri dari ibu kota dan melantik seorang anak kecil sebagai Kaisar Bing Song. Orang Mongol mengalahkan para loyalis pada pertempuran Yamen pada tahun 1279. Kaisar Song yang terakhir tenggelam, mengakhiri dinasti Song. Penaklukan Song kembali ke utara dan selatan Cina untuk pertama kalinya dalam tiga ratus tahun.\tDaerah apa yang Kublai coba tangkap dengan mempertahankan Xiangyang?\t[322]\t[lembah Sungai Yangzi]\n",
    "4\t57094ca7efce8f15003a7dd7\tSky_(United_Kingdom)\tBSkyB awalnya dikenakan biaya langganan tambahan untuk menggunakan Sky+ PVR dengan layanan mereka; melepaskan biaya untuk pelanggan yang paketnya berisi dua atau lebih saluran premium. Hal ini berubah seperti 1 Juli 2007, dan sekarang pelanggan yang memiliki Sky+ dan berlangganan untuk setiap paket langganan BSkyB mendapatkan Sky+ termasuk tanpa biaya tambahan. Pelanggan yang tidak berlangganan saluran BSkyB's masih bisa membayar biaya bulanan untuk memungkinkan fungsi Sky+. Pada bulan Januari 2010 BSkyB menghentikan Sky+ Box, membatasi standar Sky Box untuk meningkatkan hanya dan mulai mengeluarkan Sky+HD Box sebagai standar, sehingga memberikan semua pelanggan baru fungsi Sky+. Pada bulan Februari 2011 BSkyB menghentikan varian non-HD dari kotak Multiroom nya, menawarkan versi yang lebih kecil dari kotak SkyHD tanpa Sky+ fungsionalitas. Pada bulan September 2007, Sky meluncurkan kampanye iklan TV baru menargetkan Sky+ pada wanita. Pada tanggal 31 Maret 2008, Sky memiliki 3.393.000 pengguna Sky+.\tKapan Sky meluncurkan iklan TV target kampanye terhadap wanita?\t[862, 862, 963]\t[September 2007, September 2007, Maret 2008]\n",
    "5\t5710eca0a58dae1900cd6b3e\tHuguenot\tPada tahun 1700, beberapa ratus orang Prancis Huguenot bermigrasi dari Inggris ke koloni Virginia, di mana Kerajaan Inggris telah menjanjikan mereka hibah tanah di Lower Norfolk County. Ketika mereka tiba, kalangan berwenang kolonial menawarkan mereka lahan 20 mil di atas air terjun Sungai James, di desa Monacan yang ditinggalkan yang dikenal sebagai kota Manakin, sekarang di Powhatan County. Beberapa pemukim mendarat di Chesterfield County masa kini. Pada tanggal 12 Mei 1705, Majelis Umum Virginia mengadakan aksi naturalisasi 148 Huguenots masih tinggal di Manakintown. Dari 390 pemukim asli di pemukiman terpencil, banyak yang telah meninggal; lain tinggal di luar kota di peternakan dengan gaya Inggris; dan yang lain pindah ke daerah yang berbeda. Secara bertahap mereka kawin dengan tetangga Inggris mereka. Melalui abad ke - 18 dan ke - 19, keturunan Prancis bermigrasi ke barat ke Piedmont, dan melintasi Pegunungan Appalachian ke Barat dari apa yang menjadi Kentucky, Tennessee, Missouri, dan negara - negara lain. Di kawasan Manakintown, Jembatan Peringatan Huguenot di seberang Sungai James dan Jalan Huguenot dinamai untuk menghormati mereka, demikian pula banyak fitur setempat, termasuk beberapa sekolah, termasuk Sekolah Tinggi Huguenot.\tKapan para pemukim ini bersifat alami sebagai kolonis Inggris?\t[469, 476, 469]\t[12 Mei 1705, 1705, 12 Mei 1705]\n",
    "6\t57377aac1c4567190057447a\tForce\tGagasan \"force\" menyimpan maknanya dalam mekanika kuantum, meskipun satu sekarang berurusan dengan operator bukan variabel klasik dan meskipun fisika sekarang dijelaskan oleh Schrödinger persamaan bukan persamaan Newton. Hal ini memiliki konsekuensi bahwa hasil pengukuran sekarang kadang-kadang \"quantized\", i.e. mereka muncul dalam bagian diskrit. Hal ini, tentu saja, sulit untuk membayangkan dalam konteks dari \"kekuatan\". Akan tetapi, potensi V×x,y,z) atau bidang, yang dari mana kekuatan umumnya dapat diturunkan, diperlakukan mirip dengan variabel posisi klasik, yaitu...\tPersamaan apa yang menimbulkan ketegangan dalam fisika sebelum persamaan Schrodinger saat ini?\t[203, 203, 203, 203, 203]\t[persamaan Newton, persamaan Newton, persamaan Newton., persamaan Newton, persamaan Newton]\n",
    "7\t5ad03d2277cf76001a686ecf\tSouthern_California\tBanyak kesalahan mampu menghasilkan skala 6,7+ gempa bumi, seperti San Andreas Fault, yang dapat menghasilkan skala 8,0 peristiwa. Salah lainnya mencakup San Jacinto Fault, Fault Puente Hills, dan Zona Fault Elsinore. USGS telah merilis prakiraan gempa California yang merupakan model gempa bumi di California.\tApa yang UGS California Preview model?\t[285]\t[gempa bumi di.]\n",
    "8\t573407d7d058e614000b6813\tFrench_and_Indian_War\tPada bulan September 1760, dan sebelum timbul permusuhan, Gubernur Vaudreuil bernegosiasi dari Montreal sebuah penghargaan dengan Jenderal Amherst. Amherst mengabulkan permintaan Vaudreuil bahwa setiap penduduk Prancis yang memilih untuk tetap tinggal di koloni itu akan diberi kebebasan untuk terus beribadat dalam tradisi Katolik Roma, meneruskan kepemilikan tanah milik mereka, dan hak untuk tetap tidak terganggu di rumah mereka. Inggris menyediakan perawatan medis bagi tentara Prancis yang sakit dan terluka serta pasukan reguler Prancis dikembalikan ke kapal - kapal Inggris dengan persetujuan bahwa mereka tidak boleh melayani lagi dalam perang sekarang.\tDi September 1760 siapa yang bernegosiasi untuk menyerah dari Montreal?\t[58, 58, 58, 58, 58]\t[Gubernur Vaudreuil, Gubernur Vaudreuil, Gubernur Vaudreuil, Gubernur Vaudreuil, Gubernur Vaudreuil]\n",
    "9\t572871bd3acd2414000dfa04\tYuan_dynasty\tSetelah kematian Tugh Temür pada tahun 1332 dan setelah kematian Rinchinbal (Emperor Ningzong) pada tahun yang sama, Toghun Temür (Emperor Huizong), yang terakhir dari sembilan penerus Kublai Khan, dipanggil kembali dari Guangxi dan berhasil menduduki takhta. Setelah kematian El Temür, Bayan menjadi pejabat yang kuat seperti El Temür pada awal masa pemerintahannya yang panjang. Seraya Torhun Temür bertambah besar, ia mulai tidak menyetujui aturan otokratik Bayan. Pada tahun 1340, ia bersekutu dengan keponakan Bayan Toqto'a, yang sedang berselisih dengan Bayan, dan dibuang Bayan oleh kudeta. Dengan bubarnya Bayan, Meghogha merebut kekuasaan pengadilan. Administrasi pertamanya jelas menunjukkan semangat baru yang baru. Ia juga memberikan beberapa tanda awal tentang arah baru dan positif dalam pemerintahan pusat. Salah satu proyeknya yang sukses adalah menyelesaikan sejarah resmi Liao, Jin, dan Song Dinasti, yang akhirnya selesai pada tahun 1345. Namun, Heghtopha mengundurkan diri dari jabatannya dengan persetujuan dari Toghun Temür, yang menandai akhir pemerintahan pertamanya, dan ia baru dipanggil kembali pada tahun 1349.\tApa nama gaya Cina yang digunakan Rinchinbal?\t[76, 76, 76]\t[(Emperor Ningzon, (Emperor Ningzon, (Emperor Ningzon]\n",
    "\n",
    "train_df = pd.DataFrame(train)\n",
    "\n",
    "train_df['answer_start'] = [i['answer_start'] for i in train_df['answers']]\n",
    "train_df['answer_text'] = [i['text'] for i in train_df['answers']]\n",
    "\n",
    "del train_df['answers']\n",
    "     \n",
    "\n",
    "dev_df = pd.DataFrame(dev)\n",
    "\n",
    "dev_df['answer_start'] = [i['answer_start'] for i in dev_df['answers']]\n",
    "dev_df['answer_text'] = [i['text'] for i in dev_df['answers']]\n",
    "\n",
    "del dev_df['answers']\n",
    "     \n",
    "\n",
    "figsize = (10,6)\n",
    "train_df['context'].apply(len).plot.hist(title=\"Contex length histogram\", bins=20, figsize=figsize, grid=True)\n",
    "     \n",
    "<matplotlib.axes._subplots.AxesSubplot at 0x7f7794fd0f50>\n",
    "\n",
    "\n",
    "train_df['question'].apply(len).plot.hist(title=\"Question length histogram\", bins=20, figsize=figsize, grid=True)\n",
    "     \n",
    "<matplotlib.axes._subplots.AxesSubplot at 0x7ff2836c6a10>\n",
    "\n",
    "\n",
    "pd.DataFrame([len(i[0]) for i in train_df['answer_text']]).plot.hist(title=\"Answer length histogram\", bins=20, figsize=figsize, grid=True)\n",
    "     \n",
    "<matplotlib.axes._subplots.AxesSubplot at 0x7ff26fa0d2d0>\n",
    "\n",
    "\n",
    "dev_df['context'].apply(len).plot.hist(title=\"Contex length histogram\", bins=20, figsize=figsize, grid=True)\n",
    "     \n",
    "<matplotlib.axes._subplots.AxesSubplot at 0x7ff282344a50>\n",
    "\n",
    "\n",
    "dev_df['question'].apply(len).plot.hist(title=\"Question length histogram\", bins=20, figsize=figsize, grid=True)\n",
    "     \n",
    "<matplotlib.axes._subplots.AxesSubplot at 0x7ff26c586250>\n",
    "\n",
    "\n",
    "pd.DataFrame([len(i[0]) for i in dev_df['answer_text']]).plot.hist(title=\"Answer length histogram\", bins=20, figsize=figsize, grid=True)\n",
    "     \n",
    "<matplotlib.axes._subplots.AxesSubplot at 0x7ff26df3db50>\n",
    "\n",
    "Text Preprocessing\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "     \n",
    "Downloading:   0%|          | 0.00/42.0 [00:00<?, ?B/s]\n",
    "Downloading:   0%|          | 0.00/1.01k [00:00<?, ?B/s]\n",
    "Downloading:   0%|          | 0.00/234k [00:00<?, ?B/s]\n",
    "Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]\n",
    "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]\n",
    "\n",
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n",
    "     \n",
    "\n",
    "tokenizer(\"What is your name?\", \"My name is Sylvain.\")\n",
    "     \n",
    "{'input_ids': [3, 11258, 1688, 9724, 14389, 35, 4, 4470, 14389, 1688, 31710, 1589, 18, 4], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
    "\n",
    "max_length = 384 # The maximum length of a feature (question and context)\n",
    "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.\n",
    "     \n",
    "\n",
    "for i, example in enumerate(datasets[\"train\"]):\n",
    "    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > 384:\n",
    "        break\n",
    "example = datasets[\"train\"][i]\n",
    "     \n",
    "\n",
    "len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"])\n",
    "     \n",
    "415\n",
    "\n",
    "len(tokenizer(example[\"question\"], example[\"context\"], max_length=max_length, truncation=\"only_second\")[\"input_ids\"])\n",
    "     \n",
    "384\n",
    "\n",
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=doc_stride\n",
    ")\n",
    "     \n",
    "\n",
    "[len(x) for x in tokenized_example[\"input_ids\"]]\n",
    "     \n",
    "[384, 171]\n",
    "\n",
    "for x in tokenized_example[\"input_ids\"][:2]:\n",
    "    print(tokenizer.decode(x))\n",
    "     \n",
    "[CLS] beyonce menikah tahun 2008 dengan siapa? [SEP] pada tanggal 4 april 2008, beyonce menikahi jay z. dia membeberkan pernikahan mereka di depan umum dalam video montase di pesta mendengarkan album studio ketiganya, aku... sasha fierce, di sony club manhattan pada 22 oktober 2008. aku... sasha fierce dibebaskan pada tanggal 18 november 2008 di amerika serikat. album ini secara resmi memperkenalkan alter ego beyonce sasha fierce, yang dibuat pada tahun 2003 single \" crazy in love \", menjual 482. 000 salinan pada minggu pertamanya, debutnya di atas billboard 200, dan memberikan beyonce album nomor satu ketiga berturut - turut di as. album menampilkan lagu nomor satu \" single ladies ( put a ring on it ) \" dan top - lima lagu \" if i were a boy \" dan \" halo. \" mencapai pencapaiannya menjadi 100 wanita lajang terpanjang dalam karirnya, \" halo \" sukses di amerika serikat membantu beyonce mencapai lebih dari sepuluh single dalam daftar daripada wanita lain pada tahun 2000 - an. ini juga termasuk sukses \" sweet dreams \", dan singles \" diva \", \" ego \", \" broken - heardted girl \" dan \" video phone. \" video musik untuk \" single ladies \" telah rusak dan ditiru di seluruh dunia, melahirkan \" kegilaan tari pertama \" dari zaman internet menurut toronto star. video ini memenangkan beberapa penghargaan, termasuk best video pada penghargaan musik eropa mtv 2009, penghargaan mobo skotlandia 2009, dan bet awards 2009. pada penghargaan video musik mtv 2009, video itu dinominasikan untuk sembilan penghargaan, akhirnya memenangkan tiga termasuk video of the year. kegagalannya untuk memenangkan kategori video wanita terbaik, yang pergi ke penyanyi pop amerika taylor swift \" kau milik aku \", menyebabkan kanye barat mengganggu upacara dan beyonce berimprovisasi re - presentasi penghargaan swift selama pidato penerimaannya sendiri. pada maret 2009, beyonce memulai [SEP]\n",
    "[CLS] beyonce menikah tahun 2008 dengan siapa? [SEP] \" telah rusak dan ditiru di seluruh dunia, melahirkan \" kegilaan tari pertama \" dari zaman internet menurut toronto star. video ini memenangkan beberapa penghargaan, termasuk best video pada penghargaan musik eropa mtv 2009, penghargaan mobo skotlandia 2009, dan bet awards 2009. pada penghargaan video musik mtv 2009, video itu dinominasikan untuk sembilan penghargaan, akhirnya memenangkan tiga termasuk video of the year. kegagalannya untuk memenangkan kategori video wanita terbaik, yang pergi ke penyanyi pop amerika taylor swift \" kau milik aku \", menyebabkan kanye barat mengganggu upacara dan beyonce berimprovisasi re - presentasi penghargaan swift selama pidato penerimaannya sendiri. pada maret 2009, beyonce memulai i am... tour dunia, tur konser kedua di seluruh dunia, yang terdiri dari 108 pertunjukan, menggusarkan $ 119, 5 juta. [SEP]\n",
    "\n",
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    stride=doc_stride\n",
    ")\n",
    "print(tokenized_example[\"offset_mapping\"][0][:100])\n",
    "     \n",
    "[(0, 0), (0, 3), (3, 6), (6, 7), (8, 15), (16, 21), (22, 26), (27, 33), (34, 39), (39, 40), (0, 0), (0, 4), (5, 12), (13, 14), (15, 20), (21, 25), (25, 26), (27, 30), (30, 33), (33, 34), (35, 43), (44, 47), (48, 49), (49, 50), (51, 54), (55, 66), (67, 77), (78, 84), (85, 87), (88, 93), (94, 98), (99, 104), (105, 110), (111, 115), (115, 118), (119, 121), (122, 127), (128, 140), (141, 146), (147, 153), (154, 163), (163, 164), (165, 168), (168, 169), (169, 170), (170, 171), (172, 175), (175, 177), (178, 180), (180, 183), (183, 184), (184, 185), (186, 188), (189, 193), (194, 198), (199, 208), (209, 213), (214, 216), (217, 224), (225, 229), (229, 230), (231, 234), (234, 235), (235, 236), (236, 237), (238, 241), (241, 243), (244, 246), (246, 249), (249, 250), (251, 261), (262, 266), (267, 274), (275, 277), (278, 286), (287, 291), (292, 294), (295, 302), (303, 310), (310, 311), (312, 317), (318, 321), (322, 328), (329, 334), (335, 349), (350, 353), (353, 355), (356, 359), (360, 363), (363, 366), (366, 367), (368, 371), (371, 373), (374, 376), (376, 379), (379, 380), (380, 381), (382, 386), (387, 393), (394, 398)]\n",
    "\n",
    "first_token_id = tokenized_example[\"input_ids\"][0][1]\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][1]\n",
    "print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])\n",
    "     \n",
    "bey Bey\n",
    "\n",
    "sequence_ids = tokenized_example.sequence_ids()\n",
    "print(sequence_ids)\n",
    "     \n",
    "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n",
    "\n",
    "answers = example[\"answers\"]\n",
    "start_char = answers[\"answer_start\"][0]\n",
    "end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "# Start token index of the current span in the text.\n",
    "token_start_index = 0\n",
    "while sequence_ids[token_start_index] != 1:\n",
    "    token_start_index += 1\n",
    "\n",
    "# End token index of the current span in the text.\n",
    "token_end_index = len(tokenized_example[\"input_ids\"][0]) - 1\n",
    "while sequence_ids[token_end_index] != 1:\n",
    "    token_end_index -= 1\n",
    "\n",
    "# Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "offsets = tokenized_example[\"offset_mapping\"][0]\n",
    "if (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "    # Move the token_start_index and token_end_index to the two ends of the answer.\n",
    "    # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "        token_start_index += 1\n",
    "    start_position = token_start_index - 1\n",
    "    while offsets[token_end_index][1] >= end_char:\n",
    "        token_end_index -= 1\n",
    "    end_position = token_end_index + 1\n",
    "    print(start_position, end_position)\n",
    "else:\n",
    "    print(\"The answer is not in this feature.\")\n",
    "     \n",
    "21 22\n",
    "\n",
    "print(tokenizer.decode(tokenized_example[\"input_ids\"][0][start_position: end_position+1]))\n",
    "print(answers[\"text\"][0])\n",
    "     \n",
    "jay z\n",
    "Jay Z\n",
    "\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "     \n",
    "\n",
    "def prepare_train_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples\n",
    "     \n",
    "\n",
    "tokenized_datasets = datasets.map(prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names)\n",
    "     \n",
    "  0%|          | 0/131 [00:00<?, ?ba/s]\n",
    "  0%|          | 0/12 [00:00<?, ?ba/s]\n",
    "\n",
    "tokenized_datasets\n",
    "     \n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['attention_mask', 'end_positions', 'input_ids', 'start_positions', 'token_type_ids'],\n",
    "        num_rows: 131226\n",
    "    })\n",
    "    validation: Dataset({\n",
    "        features: ['attention_mask', 'end_positions', 'input_ids', 'start_positions', 'token_type_ids'],\n",
    "        num_rows: 12071\n",
    "    })\n",
    "})\n",
    "Fine Tuning Model\n",
    "\n",
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "     \n",
    "Downloading:   0%|          | 0.00/445M [00:00<?, ?B/s]\n",
    "Some weights of the model checkpoint at indolem/indobert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
    "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
    "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at indolem/indobert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"drive/MyDrive/KOPSI/test-squad\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "     \n",
    "\n",
    "from transformers import default_data_collator\n",
    "\n",
    "data_collator = default_data_collator\n",
    "     \n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "     \n",
    "\n",
    "trainer.train(\"./drive/MyDrive/KOPSI/test-squad/checkpoint-16404\")\n",
    "     \n",
    "Loading model from ./drive/MyDrive/KOPSI/test-squad/checkpoint-16404).\n",
    "You are resuming training from a checkpoint trained with 4.9.1 of Transformers but your current version is 4.9.2. This is not recommended and could yield to errors or unwanted behaviors.\n",
    "***** Running training *****\n",
    "  Num examples = 131226\n",
    "  Num Epochs = 3\n",
    "  Instantaneous batch size per device = 16\n",
    "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
    "  Gradient Accumulation steps = 1\n",
    "  Total optimization steps = 24606\n",
    "  Continuing training from checkpoint, will skip to saved global_step\n",
    "  Continuing training from epoch 2\n",
    "  Continuing training from global step 16404\n",
    "  Will skip the first 2 epochs then the first 0 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n",
    "0it [00:00, ?it/s]\n",
    "[24243/24606 2:47:11 < 07:44, 0.78 it/s, Epoch 2.96/3]\n",
    "Epoch\tTraining Loss\tValidation Loss\n",
    "[24606/24606 3:00:39, Epoch 3/3]\n",
    "Epoch\tTraining Loss\tValidation Loss\n",
    "3\t1.069500\t1.846025\n",
    "***** Running Evaluation *****\n",
    "  Num examples = 12071\n",
    "  Batch size = 16\n",
    "Saving model checkpoint to drive/MyDrive/KOPSI/test-squad/checkpoint-24606\n",
    "Configuration saved in drive/MyDrive/KOPSI/test-squad/checkpoint-24606/config.json\n",
    "Model weights saved in drive/MyDrive/KOPSI/test-squad/checkpoint-24606/pytorch_model.bin\n",
    "tokenizer config file saved in drive/MyDrive/KOPSI/test-squad/checkpoint-24606/tokenizer_config.json\n",
    "Special tokens file saved in drive/MyDrive/KOPSI/test-squad/checkpoint-24606/special_tokens_map.json\n",
    "\n",
    "\n",
    "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
    "\n",
    "\n",
    "TrainOutput(global_step=24606, training_loss=0.36222411965506063, metrics={'train_runtime': 10841.619, 'train_samples_per_second': 36.312, 'train_steps_per_second': 2.27, 'total_flos': 7.715008844873626e+16, 'train_loss': 0.36222411965506063, 'epoch': 3.0})\n",
    "\n",
    "trainer.save_model(\"drive/MyDrive/KOPSI/bert-squad-trained\")\n",
    "     \n",
    "Evaluation\n",
    "\n",
    "import torch\n",
    "\n",
    "for batch in trainer.get_eval_dataloader():\n",
    "    break\n",
    "batch = {k: v.to(trainer.args.device) for k, v in batch.items()}\n",
    "with torch.no_grad():\n",
    "    output = trainer.model(**batch)\n",
    "output.keys()\n",
    "     \n",
    "odict_keys(['loss', 'start_logits', 'end_logits'])\n",
    "\n",
    "output.start_logits.shape, output.end_logits.shape\n",
    "     \n",
    "(torch.Size([16, 384]), torch.Size([16, 384]))\n",
    "\n",
    "output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)\n",
    "     \n",
    "(tensor([ 47,  30,  75,  81, 150,  13,  41,  84, 139, 187, 112,  51,  15,  33,\n",
    "          31, 113], device='cuda:0'),\n",
    " tensor([ 47,  37,  79,  82, 150,  14,  41,  98, 144, 191, 113,  52,  22,  33,\n",
    "          31, 114], device='cuda:0'))\n",
    "\n",
    "n_best_size = 20\n",
    "     \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "# Gather the indices the best start/end logits:\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "valid_answers = []\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        if start_index <= end_index: # We need to refine that test to check the answer is inside the context\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": \"\" # We need to find a way to get back the original substring corresponding to the answer in the context\n",
    "                }\n",
    "            )\n",
    "     \n",
    "\n",
    "def prepare_validation_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # We keep the example_id that gave us this feature and we will store the offset mappings.\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "        # position is part of the context or not.\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples\n",
    "     \n",
    "\n",
    "validation_features = datasets[\"validation\"].map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"validation\"].column_names\n",
    ")\n",
    "     \n",
    "  0%|          | 0/12 [00:00<?, ?ba/s]\n",
    "\n",
    "raw_predictions = trainer.predict(validation_features)\n",
    "     \n",
    "The following columns in the test set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
    "***** Running Prediction *****\n",
    "  Num examples = 12071\n",
    "  Batch size = 16\n",
    "[755/755 05:35]\n",
    "\n",
    "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))\n",
    "     \n",
    "\n",
    "max_answer_length = 30\n",
    "     \n",
    "\n",
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "offset_mapping = validation_features[0][\"offset_mapping\"]\n",
    "# The first feature comes from the first example. For the more general case, we will need to be match the example_id to\n",
    "# an example index\n",
    "context = datasets[\"validation\"][0][\"context\"]\n",
    "\n",
    "# Gather the indices the best start/end logits:\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "valid_answers = []\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
    "        # to part of the input_ids that are not in the context.\n",
    "        if (\n",
    "            start_index >= len(offset_mapping)\n",
    "            or end_index >= len(offset_mapping)\n",
    "            or offset_mapping[start_index] is None\n",
    "            or offset_mapping[end_index] is None\n",
    "        ):\n",
    "            continue\n",
    "        # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "            continue\n",
    "        if start_index <= end_index: # We need to refine that test to check the answer is inside the context\n",
    "            start_char = offset_mapping[start_index][0]\n",
    "            end_char = offset_mapping[end_index][1]\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": context[start_char: end_char]\n",
    "                }\n",
    "            )\n",
    "\n",
    "valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
    "valid_answers\n",
    "     \n",
    "[{'score': 15.697315, 'text': 'Prancis'},\n",
    " {'score': 10.089133, 'text': 'Prancis.'},\n",
    " {'score': 9.897605, 'text': 'di Prancis'},\n",
    " {'score': 9.845985, 'text': 'sebuah kawasan di Prancis'},\n",
    " {'score': 9.531105, 'text': 'Normandia, sebuah kawasan di Prancis'},\n",
    " {'score': 9.323152,\n",
    "  'text': 'Prancis. Mereka diturunkan dari Norse (\"Norman\" berasal dari \"Norseman\") perampok dan bajak laut dari Denmark, Islandia dan Norwegia'},\n",
    " {'score': 8.580118, 'text': 'kawasan di Prancis'},\n",
    " {'score': 8.531412,\n",
    "  'text': 'Prancis. Mereka diturunkan dari Norse (\"Norman\" berasal dari \"Norseman\") perampok dan bajak laut dari Denmark'},\n",
    " {'score': 7.5963774,\n",
    "  'text': 'Prancis. Mereka diturunkan dari Norse (\"Norman\" berasal dari \"Norseman\") perampok dan bajak laut dari Denmark, Islandia'},\n",
    " {'score': 7.263671, 'text': 'Prancis. Mereka'},\n",
    " {'score': 6.988349, 'text': 'Prancis. Mereka diturunkan dari Norse (\"Norman'},\n",
    " {'score': 6.978618, 'text': 'ke Normandia, sebuah kawasan di Prancis'},\n",
    " {'score': 5.066856,\n",
    "  'text': 'Latin: Normanni) adalah orang - orang yang pada abad ke - 10 dan ke - 11 memberikan nama mereka ke Normandia, sebuah kawasan di Prancis'},\n",
    " {'score': 4.792348, 'text': ', sebuah kawasan di Prancis'},\n",
    " {'score': 4.289422, 'text': 'di Prancis.'},\n",
    " {'score': 4.2378035, 'text': 'sebuah kawasan di Prancis.'},\n",
    " {'score': 3.9229228, 'text': 'Normandia, sebuah kawasan di Prancis.'},\n",
    " {'score': 3.5234408,\n",
    "  'text': 'di Prancis. Mereka diturunkan dari Norse (\"Norman\" berasal dari \"Norseman\") perampok dan bajak laut dari Denmark, Islandia dan Norwegia'},\n",
    " {'score': 3.403661, 'text': 'di'},\n",
    " {'score': 3.352042, 'text': 'sebuah kawasan di'}]\n",
    "\n",
    "valid_answers[5]\n",
    "     \n",
    "{'score': 9.323152,\n",
    " 'text': 'Prancis. Mereka diturunkan dari Norse (\"Norman\" berasal dari \"Norseman\") perampok dan bajak laut dari Denmark, Islandia dan Norwegia'}\n",
    "\n",
    "datasets[\"validation\"][1231]\n",
    "     \n",
    "{'answers': {'answer_start': [252, 252, 252],\n",
    "  'text': ['6.000 kilometer persegi',\n",
    "   '6.000 kilometer persegi',\n",
    "   '6.000 kilometer persegi']},\n",
    " 'context': 'Lebih dari 26.000 kilometer persegi (10.000 mil persegi) lahan pertanian Victoria ditaburkan untuk biji-bijian, sebagian besar di barat negara bagian. Lebih dari 50% dari daerah ini ditabur untuk gandum, 33% untuk barli dan 7% untuk gandum. Lebih jauh 6.000 kilometer persegi (2.300 mil persegi) ditabur untuk jerami. Pada tahun 200304, para petani Victoria menghasilkan lebih dari 3 juta ton gandum dan 2 juta ton barli. Peternakan Victoria menghasilkan hampir 90% buah pir Australia dan ketiga buah apel. Hal ini juga merupakan pemimpin dalam produksi buah batu. Tanaman sayuran utama mencakup asparagus, brokoli, wortel, kentang, dan tomat. Tahun lalu, 121.200 ton pir dan 270.000 ton tomat diproduksi.',\n",
    " 'id': '570d2d68fed7b91900d45cbd',\n",
    " 'question': 'Berapa banyak lahan pertanian Victoria tumbuh jerami?',\n",
    " 'title': 'Victoria_(Australia)'}\n",
    "\n",
    "import collections\n",
    "\n",
    "examples = datasets[\"validation\"]\n",
    "features = validation_features\n",
    "\n",
    "example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "features_per_example = collections.defaultdict(list)\n",
    "for i, feature in enumerate(features):\n",
    "    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "     \n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    # Build a map example to its corresponding features.\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # The dictionaries we have to fill.\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    # Logging.\n",
    "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
    "\n",
    "    # Let's loop over all the examples!\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # Those are the indices of the features associated to the current example.\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_score = None # Only used if squad_v2 is True.\n",
    "        valid_answers = []\n",
    "        \n",
    "        context = example[\"context\"]\n",
    "        # Looping through all the features associated to the current example.\n",
    "        for feature_index in feature_indices:\n",
    "            # We grab the predictions of the model for this feature.\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
    "            # context.\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            # Update minimum null prediction.\n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
    "                    # to part of the input_ids that are not in the context.\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
    "            # failure.\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "        # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n",
    "        if not impossible_answer:\n",
    "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "        else:\n",
    "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
    "            predictions[example[\"id\"]] = answer\n",
    "\n",
    "    return predictions\n",
    "     \n",
    "\n",
    "final_predictions = postprocess_qa_predictions(datasets[\"validation\"], validation_features, raw_predictions.predictions)\n",
    "     \n",
    "Post-processing 11858 example predictions split into 12071 features.\n",
    "  0%|          | 0/11858 [00:00<?, ?it/s]\n",
    "\n",
    "def print_answer(id):\n",
    "    text = [i for i in datasets['validation'] if i['id'] == id][0]\n",
    "    print(f\"Text: {text['context']}\")\n",
    "    print(f\"Question: {text['question']}\")\n",
    "    print(f\"Answer: {final_predictions[id]}\")\n",
    "     \n",
    "\n",
    "print_answer('5ad39d53604f3c001a3fe8d1')\n",
    "     \n",
    "Text: Normans (Norman: musim hujan; Normands; Latin: Normanni) adalah orang - orang yang pada abad ke - 10 dan ke - 11 memberikan nama mereka ke Normandia, sebuah kawasan di Prancis. Mereka diturunkan dari Norse (\"Norman\" berasal dari \"Norseman\") perampok dan bajak laut dari Denmark, Islandia dan Norwegia yang, di bawah pemimpin mereka Rollo, setuju untuk bersumpah setia kepada Raja Charles III dari Francia Barat. Melalui generasi asimilasi dan mencampur dengan penduduk asli Frankis dan Romawi-Gaulis, keturunan mereka secara bertahap akan bergabung dengan Carolingian berbasis Francia. Identitas budaya dan etnis yang berbeda dari orang Norman muncul pada paruh pertama abad ke-10, dan terus berkembang selama abad - abad berikutnya.\n",
    "Question: Yang memberikan nama mereka ke Normandia di 1000 dan 1100\n",
    "Answer: Normans\n",
    "\n",
    "metric = load_metric(\"squad_v2\" if impossible_answer else \"squad\")\n",
    "     \n",
    "Downloading:   0%|          | 0.00/2.26k [00:00<?, ?B/s]\n",
    "Downloading:   0%|          | 0.00/3.18k [00:00<?, ?B/s]\n",
    "\n",
    "formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions.items()]\n",
    "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\n",
    "metric.compute(predictions=formatted_predictions, references=references)\n",
    "     \n",
    "{'HasAns_exact': 51.6191600607185,\n",
    " 'HasAns_f1': 69.09832265202414,\n",
    " 'HasAns_total': 11858,\n",
    " 'best_exact': 51.6191600607185,\n",
    " 'best_exact_thresh': 0.0,\n",
    " 'best_f1': 69.09832265202414,\n",
    " 'best_f1_thresh': 0.0,\n",
    " 'exact': 51.6191600607185,\n",
    " 'f1': 69.09832265202414,\n",
    " 'total': 11858}\n",
    "\n",
    "\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
